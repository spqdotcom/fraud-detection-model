{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845635c3-f6c7-44c9-b6c0-7119c4b5f5cd",
   "metadata": {},
   "source": [
    "# Importing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2fb4e8-1abf-4698-b796-c1d7af69ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Execute the code step by step because it takes files from other Notebooks and creates Excel files that are stored in the same folder as this Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc532f-4b62-41de-9157-c64c2b783b74",
   "metadata": {},
   "source": [
    "# Import file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88f4e7c-f590-4707-9c9f-145fe6f3994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = 'train_transaction.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6f2d50-b7ec-40f7-8fb2-685443688f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv(train_transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9038e-4350-46f1-8880-3aec08a9cfbc",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b027d31-4323-43e0-a7a5-c1282c033374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_excel(df):\n",
    "    column_summary_df = pd.DataFrame({\n",
    "        'Column Name': df.columns,\n",
    "        'Data Type': df.dtypes.values,\n",
    "        'Percentage Null': df.isnull().mean().values * 100,\n",
    "        'Unique Values': df.nunique().values,\n",
    "        'Min': np.nan,\n",
    "        'Max': np.nan,\n",
    "        'Mean': np.nan,\n",
    "        'Median': np.nan,\n",
    "        'Std': np.nan,\n",
    "        'Unique Content': ''\n",
    "    })\n",
    "\n",
    "    include_stats = input(\"Do you want to include statistical data and unique values? (yes/no): \").strip().lower()\n",
    "\n",
    "    if include_stats == 'yes':\n",
    "        for i, col in enumerate(df.columns):\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                column_summary_df.at[i, 'Min'] = df[col].min()\n",
    "                column_summary_df.at[i, 'Max'] = df[col].max()\n",
    "                column_summary_df.at[i, 'Mean'] = df[col].mean()\n",
    "                column_summary_df.at[i, 'Median'] = df[col].median()\n",
    "                column_summary_df.at[i, 'Std'] = df[col].std()\n",
    "                # Add unique values if they are less than or equal to 50\n",
    "                if df[col].nunique() <= 50:\n",
    "                    column_summary_df.at[i, 'Unique Content'] = df[col].unique()\n",
    "\n",
    "            elif pd.api.types.is_object_dtype(df[col]):\n",
    "                # Add all unique values for object data type columns\n",
    "                column_summary_df.at[i, 'Unique Content'] = df[col].unique()\n",
    "\n",
    "    save_to_excel = input(\"Do you want to save the summary to an Excel file? (yes/no): \").strip().lower()\n",
    "    \n",
    "    if save_to_excel == 'yes':\n",
    "        file_name = input(\"Enter the file name (without extension): \").strip()\n",
    "        full_file_name = f\"{file_name}.xlsx\"\n",
    "        column_summary_df.to_excel(full_file_name, index=False, engine='openpyxl')\n",
    "        print(f\"File saved as {full_file_name}\")\n",
    "    else:\n",
    "        print(\"Summary not saved to an Excel file.\")\n",
    "    \n",
    "    print(\"\\nSummary DataFrame:\")\n",
    "    print(column_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd3fa0c-2854-4363-aec2-0dc9468757db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_csv(df):\n",
    "    column_summary_df = pd.DataFrame({\n",
    "        'Column Name': df.columns,\n",
    "        'Data Type': df.dtypes.values,\n",
    "        'Percentage Null': df.isnull().mean().values * 100,\n",
    "        'Unique Values': df.nunique().values,\n",
    "        'Min': np.nan,\n",
    "        'Max': np.nan,\n",
    "        'Mean': np.nan,\n",
    "        'Median': np.nan,\n",
    "        'Std': np.nan,\n",
    "        'Unique Content': ''\n",
    "    })\n",
    "\n",
    "    include_stats = input(\"Do you want to include statistical data and unique values? (yes/no): \").strip().lower()\n",
    "\n",
    "    if include_stats == 'yes':\n",
    "        for i, col in enumerate(df.columns):\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                column_summary_df.at[i, 'Min'] = df[col].min()\n",
    "                column_summary_df.at[i, 'Max'] = df[col].max()\n",
    "                column_summary_df.at[i, 'Mean'] = df[col].mean()\n",
    "                column_summary_df.at[i, 'Median'] = df[col].median()\n",
    "                column_summary_df.at[i, 'Std'] = df[col].std()\n",
    "                # Add unique values if they are less than or equal to 50\n",
    "                if df[col].nunique() <= 50:\n",
    "                    column_summary_df.at[i, 'Unique Content'] = df[col].unique()\n",
    "\n",
    "            elif pd.api.types.is_object_dtype(df[col]):\n",
    "                # Add all unique values for object data type columns\n",
    "                column_summary_df.at[i, 'Unique Content'] = df[col].unique()\n",
    "\n",
    "    save_to_csv = input(\"Do you want to save the summary to a CSV file? (yes/no): \").strip().lower()\n",
    "    \n",
    "    if save_to_csv == 'yes':\n",
    "        file_name = input(\"Enter the file name (without extension): \").strip()\n",
    "        full_file_name = f\"{file_name}.csv\"\n",
    "        column_summary_df.to_csv(full_file_name, index=False)\n",
    "        print(f\"File saved as {full_file_name}\")\n",
    "    else:\n",
    "        print(\"Summary not saved to a CSV file.\")\n",
    "    \n",
    "    print(\"\\nSummary DataFrame:\")\n",
    "    print(column_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b1bb0b-32e2-45ad-9435-868db2c84675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 394)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da969b46-fe5f-4a31-803e-83f37fe9a05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to include statistical data and unique values? (yes/no):  yes\n",
      "Do you want to save the summary to an Excel file? (yes/no):  yes\n",
      "Enter the file name (without extension):  df_tr_before_cleaning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as df_tr_before_cleaning.xlsx\n",
      "\n",
      "Summary DataFrame:\n",
      "        Column Name Data Type  Percentage Null  Unique Values          Min  \\\n",
      "0     TransactionID     int64         0.000000         590540  2987000.000   \n",
      "1           isFraud     int64         0.000000              2        0.000   \n",
      "2     TransactionDT     int64         0.000000         573349    86400.000   \n",
      "3    TransactionAmt   float64         0.000000          20902        0.251   \n",
      "4         ProductCD    object         0.000000              5          NaN   \n",
      "..              ...       ...              ...            ...          ...   \n",
      "389            V335   float64        86.054967            672        0.000   \n",
      "390            V336   float64        86.054967            356        0.000   \n",
      "391            V337   float64        86.054967            254        0.000   \n",
      "392            V338   float64        86.054967            380        0.000   \n",
      "393            V339   float64        86.054967            334        0.000   \n",
      "\n",
      "              Max          Mean       Median           Std   Unique Content  \n",
      "0    3.577539e+06  3.282270e+06  3282269.500  1.704744e+05                   \n",
      "1    1.000000e+00  3.499001e-02        0.000  1.837546e-01           [0, 1]  \n",
      "2    1.581113e+07  7.372311e+06  7306527.500  4.617224e+06                   \n",
      "3    3.193739e+04  1.350272e+02       68.769  2.391625e+02                   \n",
      "4             NaN           NaN          NaN           NaN  [W, H, C, S, R]  \n",
      "..            ...           ...          ...           ...              ...  \n",
      "389  5.512500e+04  5.916455e+01        0.000  3.876295e+02                   \n",
      "390  5.512500e+04  2.853090e+01        0.000  2.745769e+02                   \n",
      "391  1.040600e+05  5.535242e+01        0.000  6.684868e+02                   \n",
      "392  1.040600e+05  1.511605e+02        0.000  1.095034e+03                   \n",
      "393  1.040600e+05  1.007009e+02        0.000  8.149467e+02                   \n",
      "\n",
      "[394 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "create_summary_excel(df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03cda6fd-4c5e-4595-a115-baa814fa9ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to include statistical data and unique values? (yes/no):  yes\n",
      "Do you want to save the summary to a CSV file? (yes/no):  yes\n",
      "Enter the file name (without extension):  df_tr_before_cleaning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as df_tr_before_cleaning.csv\n",
      "\n",
      "Summary DataFrame:\n",
      "        Column Name Data Type  Percentage Null  Unique Values          Min  \\\n",
      "0     TransactionID     int64         0.000000         590540  2987000.000   \n",
      "1           isFraud     int64         0.000000              2        0.000   \n",
      "2     TransactionDT     int64         0.000000         573349    86400.000   \n",
      "3    TransactionAmt   float64         0.000000          20902        0.251   \n",
      "4         ProductCD    object         0.000000              5          NaN   \n",
      "..              ...       ...              ...            ...          ...   \n",
      "389            V335   float64        86.054967            672        0.000   \n",
      "390            V336   float64        86.054967            356        0.000   \n",
      "391            V337   float64        86.054967            254        0.000   \n",
      "392            V338   float64        86.054967            380        0.000   \n",
      "393            V339   float64        86.054967            334        0.000   \n",
      "\n",
      "              Max          Mean       Median           Std   Unique Content  \n",
      "0    3.577539e+06  3.282270e+06  3282269.500  1.704744e+05                   \n",
      "1    1.000000e+00  3.499001e-02        0.000  1.837546e-01           [0, 1]  \n",
      "2    1.581113e+07  7.372311e+06  7306527.500  4.617224e+06                   \n",
      "3    3.193739e+04  1.350272e+02       68.769  2.391625e+02                   \n",
      "4             NaN           NaN          NaN           NaN  [W, H, C, S, R]  \n",
      "..            ...           ...          ...           ...              ...  \n",
      "389  5.512500e+04  5.916455e+01        0.000  3.876295e+02                   \n",
      "390  5.512500e+04  2.853090e+01        0.000  2.745769e+02                   \n",
      "391  1.040600e+05  5.535242e+01        0.000  6.684868e+02                   \n",
      "392  1.040600e+05  1.511605e+02        0.000  1.095034e+03                   \n",
      "393  1.040600e+05  1.007009e+02        0.000  8.149467e+02                   \n",
      "\n",
      "[394 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "create_summary_csv(df_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d25ef8-3535-405b-8023-5ad5b1cf6e4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b43b87-a419-4750-aa42-1e7f6a07249f",
   "metadata": {},
   "source": [
    "#### Conversion from float64 to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "677ac9f9-8bca-4283-99ad-e285e1863734",
   "metadata": {},
   "outputs": [],
   "source": [
    "float64_cols = df_tr.select_dtypes(include=['float64']).columns\n",
    "\n",
    "df_tr[float64_cols] = df_tr[float64_cols].astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49347239-5174-4a28-a405-def2bc8096f7",
   "metadata": {},
   "source": [
    "#### ProductCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df29a4e4-7ec5-4147-9531-89e1ba9f3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['ProductCD'] = df_tr['ProductCD'].map({\n",
    "    'W': 0,\n",
    "    'H': 1,\n",
    "    'C': 2,\n",
    "    'S': 3,\n",
    "    'R': 4\n",
    "}).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55465c-b035-4674-83a0-9a5e4db1d090",
   "metadata": {},
   "source": [
    "#### card4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13a81b58-cc36-4933-97bf-44597c56feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['card4'] = df_tr['card4'].map({\n",
    "    'discover': 0,\n",
    "    'mastercard': 1,\n",
    "    'visa': 2,\n",
    "    'american express': 3\n",
    "}).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc626cd-3daf-4713-920b-fb6915ac6ec0",
   "metadata": {},
   "source": [
    "#### card6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a86ffc7e-6f4e-4928-ae2a-9e433545251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['card6'] = df_tr['card6'].map({\n",
    "    'credit': 0,\n",
    "    'debit': 1,\n",
    "    'debit or credit': 2,\n",
    "    'charge card': 3\n",
    "}).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8a949-97e1-4e1f-ac96-76fae9dec809",
   "metadata": {},
   "source": [
    "#### P_emaildomain and R_emaildomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c7a6800-3200-4435-a1ae-02247a6a0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_email_domain(domain):\n",
    "    if pd.isna(domain):\n",
    "        return 'nan'\n",
    "    domain = domain.lower()\n",
    "    if 'gmail' in domain:\n",
    "        return 'gmail'\n",
    "    elif 'yahoo' in domain:\n",
    "        return 'yahoo'\n",
    "    elif 'hotmail' in domain:\n",
    "        return 'hotmail'\n",
    "    elif 'outlook' in domain:\n",
    "        return 'outlook'\n",
    "    elif 'aol' in domain:\n",
    "        return 'aol'\n",
    "    elif 'anonymous' in domain:\n",
    "        return 'anonymous'\n",
    "    elif 'icloud' in domain:\n",
    "        return 'icloud'\n",
    "    elif 'comcast' in domain:\n",
    "        return 'comcast'\n",
    "    elif 'verizon' in domain:\n",
    "        return 'verizon'\n",
    "    elif 'prodigy' in domain:\n",
    "        return 'prodigy'\n",
    "    elif 'servicios' in domain:\n",
    "        return 'servicios'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "650efec8-6a4d-408a-9774-1a3a99423581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['P_emaildomain'] = df_tr['P_emaildomain'].apply(group_email_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f2f79e5-9f46-4516-84f2-b2275efa8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['P_emaildomain'] = df_tr['P_emaildomain'].map({\n",
    "    'gmail': 0,\n",
    "    'yahoo': 1,\n",
    "    'hotmail': 2,\n",
    "    'outlook': 3,\n",
    "    'aol': 4,\n",
    "    'anonymous': 5,\n",
    "    'icloud': 6,\n",
    "    'comcast': 7,\n",
    "    'verizon': 8,\n",
    "    'prodigy': 9,\n",
    "    'servicios': 10,\n",
    "    'other': 11,\n",
    "    'nan': float('nan')\n",
    "}).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "386c09c2-1613-4fa3-bcba-52764adad756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['R_emaildomain'] = df_tr['R_emaildomain'].apply(group_email_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "884ad0ae-c8a9-4f9a-8cb4-21ffa87d92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['R_emaildomain'] = df_tr['R_emaildomain'].map({\n",
    "    'gmail': 0,\n",
    "    'yahoo': 1,\n",
    "    'hotmail': 2,\n",
    "    'outlook': 3,\n",
    "    'aol': 4,\n",
    "    'anonymous': 5,\n",
    "    'icloud': 6,\n",
    "    'comcast': 7,\n",
    "    'verizon': 8,\n",
    "    'prodigy': 9,\n",
    "    'servicios': 10,\n",
    "    'other': 11,\n",
    "    'nan': float('nan')\n",
    "}).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52343f2b-4607-41c1-9585-0afa2a910116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3815362127.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['P_emaildomain_present'] = df_tr['P_emaildomain'].notnull().astype('int')\n"
     ]
    }
   ],
   "source": [
    "df_tr['P_emaildomain_present'] = df_tr['P_emaildomain'].notnull().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c113f0c9-4270-45fa-ad62-794b0da966e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\2200280248.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['R_emaildomain_present'] = df_tr['R_emaildomain'].notnull().astype('int')\n"
     ]
    }
   ],
   "source": [
    "df_tr['R_emaildomain_present'] = df_tr['R_emaildomain'].notnull().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e045cfc-f0e8-4d55-90f1-0a6525eeaadd",
   "metadata": {},
   "source": [
    "#### Features M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92e3c2dd-060c-4c6c-aa1a-ef065a96beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_columns_tf = ['M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "for col in m_columns_tf:\n",
    "    df_tr[col] = df_tr[col].map({\n",
    "        'T': 1,\n",
    "        'F': 0\n",
    "    }).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b96f555-b99d-41ae-9e98-803218eb561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['M4'] = df_tr['M4'].map({\n",
    "    'M0': 0,\n",
    "    'M1': 1,\n",
    "    'M2': 2\n",
    "}).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0940c679-ec01-4722-aac2-edf9741e43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_columns = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22fbaadb-5dbb-4b2a-a37b-4b5a658009f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m7_m9_cols = ['M7', 'M8', 'M9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09d768b8-4277-426f-83e3-88cba1a040bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\425849077.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['M7_M9_present'] = df_tr[m7_m9_cols].notnull().any(axis=1).astype('int')\n"
     ]
    }
   ],
   "source": [
    "df_tr['M7_M9_present'] = df_tr[m7_m9_cols].notnull().any(axis=1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "754c338b-954c-471b-a7cf-3750b35b9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_m4_cols = ['M2', 'M4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c41433c-1055-42f5-acf3-eeee1521dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmar correlación de nulos para M1, M2, M3\n",
    "m1_m3_cols = ['M1', 'M2', 'M3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8e4388c-4522-4f72-8fe9-7cf0fbef55e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3109092249.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['M1_M3_present'] = df_tr[m1_m3_cols].notnull().any(axis=1).astype('int')\n"
     ]
    }
   ],
   "source": [
    "df_tr['M1_M3_present'] = df_tr[m1_m3_cols].notnull().any(axis=1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8e2d956-8074-4551-8778-2b0fbbfb3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = ['M1_present', 'M2_M3_present', 'M3_present']\n",
    "for col in redundant_cols:\n",
    "    if col in df_tr.columns:\n",
    "        df_tr = df_tr.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91371179-0fdd-4795-900f-81e2193fe3c4",
   "metadata": {},
   "source": [
    "#### Binary columns for groups of Vxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd9107e5-cbf0-4157-91a0-29d68963b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\1372960225.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['V1_V11_present'] = df_tr[v1_v11].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\1372960225.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['V12_V34_present'] = df_tr[v12_v34].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\1372960225.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['V35_V36_present'] = df_tr[v35_v36].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\1372960225.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['V37_V52_present'] = df_tr[v37_v52].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\1372960225.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['V53_V74_present'] = df_tr[v53_v74].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\1372960225.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['V75_V94_present'] = df_tr[v75_v94].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\1372960225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['V95_V137_present'] = df_tr[v95_v137].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\1372960225.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['V138_V339_present'] = df_tr[v138_v339].notnull().any(axis=1).astype('int')\n"
     ]
    }
   ],
   "source": [
    "v1_v11 = [f'V{i}' for i in range(1, 12)]\n",
    "df_tr['V1_V11_present'] = df_tr[v1_v11].notnull().any(axis=1).astype('int')\n",
    "\n",
    "v12_v34 = [f'V{i}' for i in range(12, 35)]\n",
    "df_tr['V12_V34_present'] = df_tr[v12_v34].notnull().any(axis=1).astype('int')\n",
    "\n",
    "v35_v36 = [f'V{i}' for i in range(35, 37)]\n",
    "df_tr['V35_V36_present'] = df_tr[v35_v36].notnull().any(axis=1).astype('int')\n",
    "\n",
    "v37_v52 = [f'V{i}' for i in range(37, 53)]\n",
    "df_tr['V37_V52_present'] = df_tr[v37_v52].notnull().any(axis=1).astype('int')\n",
    "\n",
    "v53_v74 = [f'V{i}' for i in range(53, 75)]\n",
    "df_tr['V53_V74_present'] = df_tr[v53_v74].notnull().any(axis=1).astype('int')\n",
    "\n",
    "v75_v94 = [f'V{i}' for i in range(75, 95)]\n",
    "df_tr['V75_V94_present'] = df_tr[v75_v94].notnull().any(axis=1).astype('int')\n",
    "\n",
    "v95_v137 = [f'V{i}' for i in range(95, 138)]\n",
    "df_tr['V95_V137_present'] = df_tr[v95_v137].notnull().any(axis=1).astype('int')\n",
    "\n",
    "v138_v339 = [f'V{i}' for i in range(138, 340)]\n",
    "df_tr['V138_V339_present'] = df_tr[v138_v339].notnull().any(axis=1).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7f508-86d4-40c8-9ce1-21162e8c20ad",
   "metadata": {},
   "source": [
    "#### Features Time (TransactionDT-D1-D15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "947f486f-860b-4bc7-a384-5be572759aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\2799687597.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['TransactionDT_days'] = df_tr['TransactionDT'] / 86400\n"
     ]
    }
   ],
   "source": [
    "df_tr['TransactionDT_days'] = df_tr['TransactionDT'] / 86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb73e4a6-0013-453d-b94d-ee82fec70776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\2715434361.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['TransactionDT_normalized'] = df_tr['TransactionDT'] - min_transaction_dt\n"
     ]
    }
   ],
   "source": [
    "min_transaction_dt = df_tr['TransactionDT'].min()\n",
    "df_tr['TransactionDT_normalized'] = df_tr['TransactionDT'] - min_transaction_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "596bb0a8-90ab-46b5-9ba6-6fce0e3795dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3551184251.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['TransactionDT_hour'] = (df_tr['TransactionDT_normalized'] % 86400) // 3600\n"
     ]
    }
   ],
   "source": [
    "df_tr['TransactionDT_hour'] = (df_tr['TransactionDT_normalized'] % 86400) // 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1518a3b7-ad60-49bd-a29a-eb97d67c61f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\636003400.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['TransactionDT_day_of_week'] = (df_tr['TransactionDT_normalized'] // 86400) % 7\n"
     ]
    }
   ],
   "source": [
    "df_tr['TransactionDT_day_of_week'] = (df_tr['TransactionDT_normalized'] // 86400) % 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab581b21-2642-4da5-a66b-6fa3ea22dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\336643310.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['TransactionDT_day_relative'] = df_tr['TransactionDT_normalized'] // 86400\n"
     ]
    }
   ],
   "source": [
    "df_tr['TransactionDT_day_relative'] = df_tr['TransactionDT_normalized'] // 86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4972b180-1206-4cd5-9fd8-fe48632225dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['TransactionDT_days'] = df_tr['TransactionDT_days'].astype('float32')\n",
    "df_tr['TransactionDT_normalized'] = df_tr['TransactionDT_normalized'].astype('float32')\n",
    "df_tr['TransactionDT_hour'] = df_tr['TransactionDT_hour'].astype('float32')\n",
    "df_tr['TransactionDT_day_of_week'] = df_tr['TransactionDT_day_of_week'].astype('float32')\n",
    "df_tr['TransactionDT_day_relative'] = df_tr['TransactionDT_day_relative'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d182950-c5ad-4929-ac9d-7b8745c862d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3099098235.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D2_minus_D1'] = df_tr['D2'] - df_tr['D1']\n"
     ]
    }
   ],
   "source": [
    "df_tr['D2_minus_D1'] = df_tr['D2'] - df_tr['D1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6373fd4-7972-4e25-ab7c-b44494b85c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\385601232.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D2_over_D1'] = df_tr['D2'] / df_tr['D1'].replace(0, np.nan)\n"
     ]
    }
   ],
   "source": [
    "df_tr['D2_over_D1'] = df_tr['D2'] / df_tr['D1'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34f9b01c-8c0b-4df7-a1df-77f591900b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\363155881.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D1_frequency'] = 1 / df_tr['D1'].replace(0, np.nan)\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\363155881.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D2_frequency'] = 1 / df_tr['D2'].replace(0, np.nan)\n"
     ]
    }
   ],
   "source": [
    "df_tr['D1_frequency'] = 1 / df_tr['D1'].replace(0, np.nan)\n",
    "df_tr['D2_frequency'] = 1 / df_tr['D2'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f459f8f-4744-4e45-8366-86407a507411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['D2_minus_D1'] = df_tr['D2_minus_D1'].astype('float32')\n",
    "df_tr['D2_over_D1'] = df_tr['D2_over_D1'].astype('float32')\n",
    "df_tr['D1_frequency'] = df_tr['D1_frequency'].astype('float32')\n",
    "df_tr['D2_frequency'] = df_tr['D2_frequency'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5edee7e-a379-4584-95a3-5f4527efc63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\508760420.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D6_D9_present'] = df_tr[d6_d9].notnull().any(axis=1).astype('int')\n"
     ]
    }
   ],
   "source": [
    "d6_d9 = ['D6', 'D8', 'D9']\n",
    "df_tr['D6_D9_present'] = df_tr[d6_d9].notnull().any(axis=1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9032deb6-c9ae-4204-9a2e-9dcb4728e6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\2737083294.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D12_D14_present'] = df_tr[d12_d14].notnull().any(axis=1).astype('int')\n"
     ]
    }
   ],
   "source": [
    "d12_d14 = ['D12', 'D13', 'D14']\n",
    "df_tr['D12_D14_present'] = df_tr[d12_d14].notnull().any(axis=1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1139662f-ed6c-4c8b-9523-7f27f57fc270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\1784596603.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D2_D3_D11_present'] = df_tr[d2_d3_d11].notnull().any(axis=1).astype('int')\n"
     ]
    }
   ],
   "source": [
    "d2_d3_d11 = ['D2', 'D3', 'D11']\n",
    "df_tr['D2_D3_D11_present'] = df_tr[d2_d3_d11].notnull().any(axis=1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45ad2462-e516-4907-969a-9ccb4f5dc71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3994410773.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D1_present'] = df_tr['D1'].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3994410773.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D4_present'] = df_tr['D4'].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3994410773.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D5_present'] = df_tr['D5'].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3994410773.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D7_present'] = df_tr['D7'].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3994410773.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D10_present'] = df_tr['D10'].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3994410773.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['D15_present'] = df_tr['D15'].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\3994410773.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['dist2_present'] = df_tr['dist2'].notnull().astype('int')\n"
     ]
    }
   ],
   "source": [
    "df_tr['D1_present'] = df_tr['D1'].notnull().astype('int')\n",
    "df_tr['D4_present'] = df_tr['D4'].notnull().astype('int')\n",
    "df_tr['D5_present'] = df_tr['D5'].notnull().astype('int')\n",
    "df_tr['D7_present'] = df_tr['D7'].notnull().astype('int')\n",
    "df_tr['D10_present'] = df_tr['D10'].notnull().astype('int')\n",
    "df_tr['D15_present'] = df_tr['D15'].notnull().astype('int')\n",
    "df_tr['dist2_present'] = df_tr['dist2'].notnull().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a0f74-08d6-4dd8-9eed-94e515b0dafa",
   "metadata": {},
   "source": [
    "#### Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d28b174b-d722-4331-92f2-a630cceae4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_7560\\2732147162.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_tr['addr_present'] = df_tr[['addr1', 'addr2']].notnull().any(axis=1).astype('int')\n"
     ]
    }
   ],
   "source": [
    "df_tr['addr_present'] = df_tr[['addr1', 'addr2']].notnull().any(axis=1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "12b7758c-f317-4297-bcbb-6b408cd9c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = ['id_11_38_present', 'id_02_DeviceType_present', 'id_31_present',\n",
    "               'id_17_20_present', 'id_05_06_present', 'id_13_present', 'id_16_present',\n",
    "               'id_03_04_present', 'id_09_10_present', 'id_14_present',\n",
    "               'id_18_present', 'id_07_27_present', 'id_24_present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "806c6e66-7741-449a-937f-b251ea549dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_fe = df_tr.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6d0b0-cf79-4268-8561-a82dff178147",
   "metadata": {},
   "source": [
    "#### Eliminate Vxxx columns with low importance (importance <= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28d15c44-6e45-4148-a12b-5ca564affe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = {\n",
    "    'R_emaildomain': 115,\n",
    "    'TransactionAmt': 111,\n",
    "    'C1': 110,\n",
    "    'C14': 96,\n",
    "    'card2': 93,\n",
    "    'card3': 83,\n",
    "    'card1': 81,\n",
    "    'D8': 79,\n",
    "    'C13': 75,\n",
    "    'ProductCD': 69,\n",
    "    'P_emaildomain': 69,\n",
    "    'TransactionDT': 63,\n",
    "    'id_02': 57,\n",
    "    'TransactionDT_day_relative': 57,\n",
    "    'D2': 55,\n",
    "    'id_20': 53,\n",
    "    'V156': 49,\n",
    "    'card6': 43,\n",
    "    'id_30': 42,\n",
    "    'V87': 37,\n",
    "    'V258': 36,\n",
    "    'V165': 34,\n",
    "    'C11': 33,\n",
    "    'addr1': 31,\n",
    "    'id_06': 28,\n",
    "    'id_14': 28,\n",
    "    'D4': 28,\n",
    "    'C3': 26,\n",
    "    'id_09': 25,\n",
    "    'V45': 25,\n",
    "    'D13': 24,\n",
    "    'id_01': 23,\n",
    "    'dist2': 22,\n",
    "    'id_17': 21,\n",
    "    'id_33': 21,\n",
    "    'D15': 20,\n",
    "    'V55': 20,\n",
    "    'TransactionDT_hour': 19,\n",
    "    'D3': 18,\n",
    "    'D14': 18,\n",
    "    'V189': 18,\n",
    "    'id_31': 17,\n",
    "    'DeviceInfo': 17,\n",
    "    'D6': 17,\n",
    "    'id_19': 16,\n",
    "    'C2': 16,\n",
    "    'card5': 15,\n",
    "    'D2_frequency': 15,\n",
    "    'id_03': 13,\n",
    "    'id_07': 13,\n",
    "    'id_18': 13,\n",
    "    'V308': 13,\n",
    "    'V56': 12,\n",
    "    'V58': 12,\n",
    "    'V310': 12,\n",
    "    'V314': 12,\n",
    "    'V315': 12,\n",
    "    'TransactionDT_days': 12,\n",
    "    'card4': 10,\n",
    "    'V139': 10,\n",
    "    'V251': 10,\n",
    "    'V261': 10,\n",
    "    'C12': 9,\n",
    "    'V67': 9,\n",
    "    'V74': 9,\n",
    "    'V78': 9,\n",
    "    'V207': 9,\n",
    "    'id_36': 8,\n",
    "    'V37': 8,\n",
    "    'V234': 8,\n",
    "    'id_10': 7,\n",
    "    'V149': 7,\n",
    "    'V152': 7,\n",
    "    'C4': 6,\n",
    "    'C8': 6,\n",
    "    'V143': 6,\n",
    "    'V158': 6,\n",
    "    'V169': 6,\n",
    "    'V206': 6,\n",
    "    'V245': 6,\n",
    "    'V332': 6,\n",
    "    'V333': 6,\n",
    "    'V338': 6,\n",
    "    'id_04': 5,\n",
    "    'id_32': 5,\n",
    "    'V57': 5,\n",
    "    'V137': 5,\n",
    "    'V145': 5,\n",
    "    'V159': 5,\n",
    "    'V162': 5,\n",
    "    'V166': 5,\n",
    "    'V217': 5,\n",
    "    'V221': 5,\n",
    "    'V224': 5,\n",
    "    'V263': 5,\n",
    "    'V280': 5,\n",
    "    'V309': 5,\n",
    "    'V336': 5,\n",
    "    'D1_frequency': 5,\n",
    "    'id_13': 4,\n",
    "    'D5': 4,\n",
    "    'V38': 4,\n",
    "    'V150': 4,\n",
    "    'V157': 4,\n",
    "    'V160': 4,\n",
    "    'V170': 4,\n",
    "    'V172': 4,\n",
    "    'V187': 4,\n",
    "    'V192': 4,\n",
    "    'V205': 4,\n",
    "    'V209': 4,\n",
    "    'V259': 4,\n",
    "    'V266': 4,\n",
    "    'V281': 4,\n",
    "    'V293': 4,\n",
    "    'V294': 4,\n",
    "    'V335': 4,\n",
    "    'V339': 4,\n",
    "    'P_emaildomain_present': 4,\n",
    "    'D7': 3,\n",
    "    'D9': 3,\n",
    "    'D10': 3,\n",
    "    'D12': 3,\n",
    "    'V44': 3,\n",
    "    'V52': 3,\n",
    "    'V77': 3,\n",
    "    'V79': 3,\n",
    "    'V99': 3,\n",
    "    'V109': 3,\n",
    "    'V130': 3,\n",
    "    'V161': 3,\n",
    "    'V164': 3,\n",
    "    'V201': 3,\n",
    "    'V208': 3,\n",
    "    'V229': 3,\n",
    "    'V243': 3,\n",
    "    'V257': 3,\n",
    "    'V271': 3,\n",
    "    'V312': 3,\n",
    "    'V313': 3,\n",
    "    'V324': 3,\n",
    "    'V327': 3,\n",
    "    'TransactionDT_day_of_week': 3,\n",
    "    'id_34': 2,\n",
    "    'C6': 2,\n",
    "    'C7': 2,\n",
    "    'V25': 2,\n",
    "    'V42': 2,\n",
    "    'V43': 2,\n",
    "    'V47': 2,\n",
    "    'V51': 2,\n",
    "    'V63': 2,\n",
    "    'V64': 2,\n",
    "    'V66': 2,\n",
    "    'V71': 2,\n",
    "    'V72': 2,\n",
    "    'V73': 2,\n",
    "    'V81': 2,\n",
    "    'V83': 2,\n",
    "    'V84': 2,\n",
    "    'V127': 2,\n",
    "    'V128': 2,\n",
    "    'V135': 2,\n",
    "    'V140': 2,\n",
    "    'V178': 2,\n",
    "    'V213': 2,\n",
    "    'V228': 2,\n",
    "    'V230': 2,\n",
    "    'V232': 2,\n",
    "    'V256': 2,\n",
    "    'V264': 2,\n",
    "    'V265': 2,\n",
    "    'V267': 2,\n",
    "    'V268': 2,\n",
    "    'V270': 2,\n",
    "    'V279': 2,\n",
    "    'V283': 2,\n",
    "    'V285': 2,\n",
    "    'V290': 2,\n",
    "    'V311': 2,\n",
    "    'V317': 2,\n",
    "    'V323': 2,\n",
    "    'V330': 2,\n",
    "    'D2_over_D1': 2,\n",
    "    'id_08': 1,\n",
    "    'id_11': 1,\n",
    "    'id_21': 1,\n",
    "    'id_24': 1,\n",
    "    'DeviceType': 1,\n",
    "    'V15': 1,\n",
    "    'V18': 1,\n",
    "    'V23': 1,\n",
    "    'V39': 1,\n",
    "    'V40': 1,\n",
    "    'V53': 1,\n",
    "    'V94': 1,\n",
    "    'V102': 1,\n",
    "    'V123': 1,\n",
    "    'V133': 1,\n",
    "    'V136': 1,\n",
    "    'V151': 1,\n",
    "    'V154': 1,\n",
    "    'V155': 1,\n",
    "    'V175': 1,\n",
    "    'V181': 1,\n",
    "    'V182': 1,\n",
    "    'V184': 1,\n",
    "    'V185': 1,\n",
    "    'V188': 1,\n",
    "    'V204': 1,\n",
    "    'V212': 1,\n",
    "    'V223': 1,\n",
    "    'V225': 1,\n",
    "    'V231': 1,\n",
    "    'V233': 1,\n",
    "    'V238': 1,\n",
    "    'V242': 1,\n",
    "    'V244': 1,\n",
    "    'V246': 1,\n",
    "    'V248': 1,\n",
    "    'V262': 1,\n",
    "    'V272': 1,\n",
    "    'V274': 1,\n",
    "    'V278': 1,\n",
    "    'V282': 1,\n",
    "    'V296': 1,\n",
    "    'V300': 1,\n",
    "    'V303': 1,\n",
    "    'V319': 1,\n",
    "    'V328': 1,\n",
    "    'V331': 1,\n",
    "    'V334': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "420372a4-6603-4f6d-9d02-bd9ede51d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_importance_v_cols = [f'V{i}' for i in range(1, 340) if f'V{i}' in df_tr_fe.columns and f'V{i}' in feature_importances and feature_importances[f'V{i}'] <= 2]\n",
    "df_tr_fe = df_tr_fe.drop(columns=low_importance_v_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4500aa8-f0a1-4e15-8ccd-fa3453ade02b",
   "metadata": {},
   "source": [
    "#### Convert float64 columns to float32 and int64 columns to int32 to reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "86e4aa85-a218-4cb4-a04f-d64b08b8f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_tr_fe.select_dtypes(include=['float64']).columns:\n",
    "    df_tr_fe[col] = df_tr_fe[col].astype('float32')\n",
    "for col in df_tr_fe.select_dtypes(include=['int64']).columns:\n",
    "    df_tr_fe[col] = df_tr_fe[col].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aecddd-5d76-4281-a3f5-e44526a23d17",
   "metadata": {},
   "source": [
    "#### 1. Proportion of frauds by R_emaildomain (high-risk domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "87946f1b-8db9-4019-9601-58d8b349a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rate_by_remail = df_tr_fe.groupby('R_emaildomain')['isFraud'].mean().to_dict()\n",
    "df_tr_fe['Fraud_Rate_R_emaildomain'] = df_tr_fe['R_emaildomain'].map(fraud_rate_by_remail).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df7ca7-4bd7-4d6a-8d51-b5f3c777d09f",
   "metadata": {},
   "source": [
    "#### 2. Difference between TransactionAmt and average per card1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b0496634-9c90-4222-8b90-83162e79cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_amt_by_card1 = df_tr_fe.groupby('card1')['TransactionAmt'].mean()\n",
    "df_tr_fe['TransactionAmt_Diff_Avg_card1'] = (df_tr_fe['TransactionAmt'] - df_tr_fe['card1'].map(avg_amt_by_card1)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca779bab-403e-4081-91f4-f161a4b98557",
   "metadata": {},
   "source": [
    "#### 3. Extreme TransactionAmt Indicator by card1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "28ed4732-2e24-46d1-b1e2-a331f0aba262",
   "metadata": {},
   "outputs": [],
   "source": [
    "amt_percentile_90_by_card1 = df_tr_fe.groupby('card1')['TransactionAmt'].quantile(0.90)\n",
    "df_tr_fe['TransactionAmt_High_card1'] = (df_tr_fe['TransactionAmt'] > df_tr_fe['card1'].map(amt_percentile_90_by_card1)).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3724394-5943-4b29-8ac8-ee0cc0de8935",
   "metadata": {},
   "source": [
    "#### 4. Ratio C1 / C14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "af34261d-7879-4803-a9c6-1a6c1dcd2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c14_threshold = df_tr_fe['C14'].quantile(0.90, interpolation='nearest')\n",
    "df_tr_fe['C14_High'] = (df_tr_fe['C14'] > c14_threshold).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff880df-3a55-45f1-976d-b7ca03a8f6e5",
   "metadata": {},
   "source": [
    "#### 6. Frequency of transactions per card1 in the last day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "97cfffca-cfbb-41ab-ac3b-15cc3d828981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_fe['Transactions_Last_Day'] = np.zeros(len(df_tr_fe), dtype=np.float32)\n",
    "grouped = df_tr_fe.groupby('card1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cc7044de-1dba-4371-81c3-6e7dc2745b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for card, group in grouped:\n",
    "    # Ordenar el grupo por TransactionDT\n",
    "    group = group.sort_values('TransactionDT')\n",
    "    transaction_dt = group['TransactionDT'].values\n",
    "    indices = group.index\n",
    "    counts = np.zeros(len(group), dtype=np.float32)\n",
    "    \n",
    "    # Para cada transacción, contar cuántas transacciones previas están dentro de las últimas 24 horas\n",
    "    for i in range(len(group)):\n",
    "        # Ventana de 24 horas (86400 segundos)\n",
    "        window_start = transaction_dt[i] - 86400\n",
    "        # Contar transacciones dentro de la ventana\n",
    "        counts[i] = np.sum((transaction_dt >= window_start) & (transaction_dt <= transaction_dt[i]))\n",
    "    \n",
    "    # Asignar los valores al DataFrame original usando los índices del grupo\n",
    "    df_tr_fe.loc[indices, 'Transactions_Last_Day'] = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77680ea8-e08b-4fc8-b403-320bab422fb5",
   "metadata": {},
   "source": [
    "#### 7. Difference D8 - D1 (additional temporary delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5ab66eb0-8c1e-40e5-a942-dc62a237b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_fe['D8_minus_D1'] = (df_tr_fe['D8'] - df_tr_fe['D1']).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f040eb7-f752-43eb-9017-50a45cbf2626",
   "metadata": {},
   "source": [
    "#### 8. Sum of V156-V166 (major group of Vxxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7f503b31-3469-45a3-8fba-6f5cdc2848ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "v156_166 = [f'V{i}' for i in range(156, 167)]\n",
    "df_tr_fe['V156_V166_Sum'] = df_tr_fe[v156_166].sum(axis=1, skipna=True).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df6871-b132-4289-aec4-c0fc5d960feb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Output after cleaning ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "feaf780d-1f6e-4e43-b695-a17743889566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to include statistical data and unique values? (yes/no):  yes\n",
      "Do you want to save the summary to an Excel file? (yes/no):  yes\n",
      "Enter the file name (without extension):  df_tr_after_cleaning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as df_tr_after_cleaning.xlsx\n",
      "\n",
      "Summary DataFrame:\n",
      "                   Column Name Data Type  Percentage Null  Unique Values  \\\n",
      "0                TransactionID     int32          0.00000         590540   \n",
      "1                      isFraud     int32          0.00000              2   \n",
      "2                TransactionDT     int32          0.00000         573349   \n",
      "3               TransactionAmt   float32          0.00000          20902   \n",
      "4                    ProductCD   float32          0.00000              5   \n",
      "..                         ...       ...              ...            ...   \n",
      "348  TransactionAmt_High_card1     int32          0.00000              2   \n",
      "349                   C14_High     int32          0.00000              2   \n",
      "350      Transactions_Last_Day   float32          0.00000            881   \n",
      "351                D8_minus_D1   float32         87.33024          17156   \n",
      "352              V156_V166_Sum   float32          0.00000          13023   \n",
      "\n",
      "              Min           Max          Mean        Median           Std  \\\n",
      "0    2.987000e+06  3.577539e+06  3.282270e+06  3.282270e+06  1.704744e+05   \n",
      "1    0.000000e+00  1.000000e+00  3.499001e-02  0.000000e+00  1.837546e-01   \n",
      "2    8.640000e+04  1.581113e+07  7.372311e+06  7.306528e+06  4.617224e+06   \n",
      "3    2.510000e-01  3.193739e+04  1.350272e+02  6.876900e+01  2.391627e+02   \n",
      "4    0.000000e+00  4.000000e+00  6.024012e-01  0.000000e+00  1.167801e+00   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "348  0.000000e+00  1.000000e+00  9.884174e-02  0.000000e+00  2.984497e-01   \n",
      "349  0.000000e+00  1.000000e+00  9.321130e-02  0.000000e+00  2.907286e-01   \n",
      "350  1.000000e+00  8.810000e+02  1.980424e+01  6.000000e+00  4.563472e+01   \n",
      "351 -6.374167e+02  1.707792e+03  1.091971e+02  1.304167e+01  2.499484e+02   \n",
      "352  0.000000e+00  6.797224e+05  7.447824e+03  0.000000e+00  6.016157e+04   \n",
      "\n",
      "                Unique Content  \n",
      "0                               \n",
      "1                       [0, 1]  \n",
      "2                               \n",
      "3                               \n",
      "4    [0.0, 1.0, 2.0, 3.0, 4.0]  \n",
      "..                         ...  \n",
      "348                     [0, 1]  \n",
      "349                     [0, 1]  \n",
      "350                             \n",
      "351                             \n",
      "352                             \n",
      "\n",
      "[353 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "create_summary_excel(df_tr_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6349b332-e1bf-4526-8b17-337d7decaa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to include statistical data and unique values? (yes/no):  yes\n",
      "Do you want to save the summary to a CSV file? (yes/no):  yes\n",
      "Enter the file name (without extension):  df_tr_after_cleaning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as df_tr_after_cleaning.csv\n",
      "\n",
      "Summary DataFrame:\n",
      "                   Column Name Data Type  Percentage Null  Unique Values  \\\n",
      "0                TransactionID     int32          0.00000         590540   \n",
      "1                      isFraud     int32          0.00000              2   \n",
      "2                TransactionDT     int32          0.00000         573349   \n",
      "3               TransactionAmt   float32          0.00000          20902   \n",
      "4                    ProductCD   float32          0.00000              5   \n",
      "..                         ...       ...              ...            ...   \n",
      "348  TransactionAmt_High_card1     int32          0.00000              2   \n",
      "349                   C14_High     int32          0.00000              2   \n",
      "350      Transactions_Last_Day   float32          0.00000            881   \n",
      "351                D8_minus_D1   float32         87.33024          17156   \n",
      "352              V156_V166_Sum   float32          0.00000          13023   \n",
      "\n",
      "              Min           Max          Mean        Median           Std  \\\n",
      "0    2.987000e+06  3.577539e+06  3.282270e+06  3.282270e+06  1.704744e+05   \n",
      "1    0.000000e+00  1.000000e+00  3.499001e-02  0.000000e+00  1.837546e-01   \n",
      "2    8.640000e+04  1.581113e+07  7.372311e+06  7.306528e+06  4.617224e+06   \n",
      "3    2.510000e-01  3.193739e+04  1.350272e+02  6.876900e+01  2.391627e+02   \n",
      "4    0.000000e+00  4.000000e+00  6.024012e-01  0.000000e+00  1.167801e+00   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "348  0.000000e+00  1.000000e+00  9.884174e-02  0.000000e+00  2.984497e-01   \n",
      "349  0.000000e+00  1.000000e+00  9.321130e-02  0.000000e+00  2.907286e-01   \n",
      "350  1.000000e+00  8.810000e+02  1.980424e+01  6.000000e+00  4.563472e+01   \n",
      "351 -6.374167e+02  1.707792e+03  1.091971e+02  1.304167e+01  2.499484e+02   \n",
      "352  0.000000e+00  6.797224e+05  7.447824e+03  0.000000e+00  6.016157e+04   \n",
      "\n",
      "                Unique Content  \n",
      "0                               \n",
      "1                       [0, 1]  \n",
      "2                               \n",
      "3                               \n",
      "4    [0.0, 1.0, 2.0, 3.0, 4.0]  \n",
      "..                         ...  \n",
      "348                     [0, 1]  \n",
      "349                     [0, 1]  \n",
      "350                             \n",
      "351                             \n",
      "352                             \n",
      "\n",
      "[353 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "create_summary_csv(df_tr_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a6792d-60f8-43a7-ab27-d89114e9e137",
   "metadata": {},
   "source": [
    "# Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ae2c1-2f68-4feb-8aa8-92ead58f92b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91274c5-4471-43b3-83ea-6ee022108908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "class TransactionCleaner:\n",
    "    def __init__(self):\n",
    "        self.avg_amt_by_card1 = None  # Para almacenar el promedio de TransactionAmt por card1\n",
    "        self.amt_percentile_90_by_card1 = None  # Para almacenar el percentil 90 de TransactionAmt por card1\n",
    "        self.fraud_rate_by_remail = None  # Para almacenar la tasa de fraude por R_emaildomain\n",
    "        self.c14_threshold = None  # Para almacenar el umbral de C14 (percentil 90)\n",
    "\n",
    "    def group_email_domain(self, domain):\n",
    "        if pd.isna(domain):\n",
    "            return 'nan'\n",
    "        domain = domain.lower()\n",
    "        if 'gmail' in domain:\n",
    "            return 'gmail'\n",
    "        elif 'yahoo' in domain:\n",
    "            return 'yahoo'\n",
    "        elif 'hotmail' in domain:\n",
    "            return 'hotmail'\n",
    "        elif 'outlook' in domain:\n",
    "            return 'outlook'\n",
    "        elif 'aol' in domain:\n",
    "            return 'aol'\n",
    "        elif 'anonymous' in domain:\n",
    "            return 'anonymous'\n",
    "        elif 'icloud' in domain:\n",
    "            return 'icloud'\n",
    "        elif 'comcast' in domain:\n",
    "            return 'comcast'\n",
    "        elif 'verizon' in domain:\n",
    "            return 'verizon'\n",
    "        elif 'prodigy' in domain:\n",
    "            return 'prodigy'\n",
    "        elif 'servicios' in domain:\n",
    "            return 'servicios'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "    def fit(self, df):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"Expected a pandas DataFrame, but got {type(df)}\")\n",
    "\n",
    "        df = df.copy()\n",
    "        df.columns = df.columns.str.replace('-', '_')\n",
    "\n",
    "        # Transformar R_emaildomain a valores numéricos antes de calcular la tasa de fraude\n",
    "        email_mapping = {\n",
    "            'gmail': 0, 'yahoo': 1, 'hotmail': 2, 'outlook': 3, 'aol': 4,\n",
    "            'anonymous': 5, 'icloud': 6, 'comcast': 7, 'verizon': 8,\n",
    "            'prodigy': 9, 'servicios': 10, 'other': 11, 'nan': float('nan')\n",
    "        }\n",
    "        df['R_emaildomain'] = df['R_emaildomain'].apply(self.group_email_domain)\n",
    "        df['R_emaildomain'] = df['R_emaildomain'].map(email_mapping).astype('float32')\n",
    "\n",
    "        # 1. Promedio de TransactionAmt por card1 (sin manejo explícito de nulos, como en el script manual)\n",
    "        self.avg_amt_by_card1 = df.groupby('card1')['TransactionAmt'].mean()\n",
    "\n",
    "        # 2. Percentil 90 de TransactionAmt por card1\n",
    "        self.amt_percentile_90_by_card1 = df.groupby('card1')['TransactionAmt'].quantile(0.90)\n",
    "\n",
    "        # 3. Tasa de fraude por R_emaildomain\n",
    "        self.fraud_rate_by_remail = df.groupby('R_emaildomain')['isFraud'].mean().to_dict()\n",
    "\n",
    "        # 4. Umbral de C14 (percentil 90)\n",
    "        self.c14_threshold = df['C14'].quantile(0.90, interpolation='nearest')\n",
    "\n",
    "        # Debug: Imprimir estadísticas calculadas\n",
    "        print(\"\\nEstadísticas calculadas en fit:\")\n",
    "        print(\"avg_amt_by_card1:\")\n",
    "        print(self.avg_amt_by_card1.describe())\n",
    "        print(\"\\namt_percentile_90_by_card1:\")\n",
    "        print(self.amt_percentile_90_by_card1.describe())\n",
    "        print(\"\\nfraud_rate_by_remail:\")\n",
    "        print(self.fraud_rate_by_remail)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"Expected a pandas DataFrame, but got {type(df)}\")\n",
    "\n",
    "        df = df.copy()\n",
    "        df.columns = df.columns.str.replace('-', '_')\n",
    "\n",
    "        # 1. Convertir columnas float64 a float32\n",
    "        float64_cols = df.select_dtypes(include=['float64']).columns\n",
    "        df[float64_cols] = df[float64_cols].astype('float32')\n",
    "\n",
    "        # 2. Transformar ProductCD\n",
    "        df['ProductCD'] = df['ProductCD'].map({\n",
    "            'W': 0,\n",
    "            'H': 1,\n",
    "            'C': 2,\n",
    "            'S': 3,\n",
    "            'R': 4\n",
    "        }).astype('float32')\n",
    "\n",
    "        # 3. Transformar card4\n",
    "        df['card4'] = df['card4'].map({\n",
    "            'discover': 0,\n",
    "            'mastercard': 1,\n",
    "            'visa': 2,\n",
    "            'american express': 3\n",
    "        }).astype('float32')\n",
    "\n",
    "        # 4. Transformar card6\n",
    "        df['card6'] = df['card6'].map({\n",
    "            'credit': 0,\n",
    "            'debit': 1,\n",
    "            'debit or credit': 2,\n",
    "            'charge card': 3\n",
    "        }).astype('float32')\n",
    "\n",
    "        # 5. Transformar P_emaildomain y R_emaildomain\n",
    "        email_mapping = {\n",
    "            'gmail': 0, 'yahoo': 1, 'hotmail': 2, 'outlook': 3, 'aol': 4,\n",
    "            'anonymous': 5, 'icloud': 6, 'comcast': 7, 'verizon': 8,\n",
    "            'prodigy': 9, 'servicios': 10, 'other': 11, 'nan': float('nan')\n",
    "        }\n",
    "        df['P_emaildomain'] = df['P_emaildomain'].apply(self.group_email_domain)\n",
    "        df['P_emaildomain'] = df['P_emaildomain'].map(email_mapping).astype('float32')\n",
    "\n",
    "        df['R_emaildomain'] = df['R_emaildomain'].apply(self.group_email_domain)\n",
    "        df['R_emaildomain'] = df['R_emaildomain'].map(email_mapping).astype('float32')\n",
    "\n",
    "        # Crear columnas binarias para indicar presencia\n",
    "        df['P_emaildomain_present'] = df['P_emaildomain'].notnull().astype('int')\n",
    "        df['R_emaildomain_present'] = df['R_emaildomain'].notnull().astype('int')\n",
    "\n",
    "        # 6. Transformar columnas M\n",
    "        m_columns_tf = ['M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "        for col in m_columns_tf:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].map({\n",
    "                    'T': 1,\n",
    "                    'F': 0\n",
    "                }).astype('float32')\n",
    "\n",
    "        if 'M4' in df.columns:\n",
    "            df['M4'] = df['M4'].map({\n",
    "                'M0': 0,\n",
    "                'M1': 1,\n",
    "                'M2': 2\n",
    "            }).astype('float32')\n",
    "\n",
    "        # Crear columnas binarias para grupos de M\n",
    "        m7_m9_cols = ['M7', 'M8', 'M9']\n",
    "        if all(col in df.columns for col in m7_m9_cols):\n",
    "            df['M7_M9_present'] = df[m7_m9_cols].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        m1_m3_cols = ['M1', 'M2', 'M3']\n",
    "        if all(col in df.columns for col in m1_m3_cols):\n",
    "            df['M1_M3_present'] = df[m1_m3_cols].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        # Eliminar columnas redundantes si existen\n",
    "        redundant_cols = ['M1_present', 'M2_M3_present', 'M3_present']\n",
    "        df = df.drop(columns=[col for col in redundant_cols if col in df.columns])\n",
    "\n",
    "        # 7. Crear columnas binarias para grupos de Vxxx\n",
    "        v1_v11 = [f'V{i}' for i in range(1, 12) if f'V{i}' in df.columns]\n",
    "        if v1_v11:\n",
    "            df['V1_V11_present'] = df[v1_v11].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        v12_v34 = [f'V{i}' for i in range(12, 35) if f'V{i}' in df.columns]\n",
    "        if v12_v34:\n",
    "            df['V12_V34_present'] = df[v12_v34].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        v35_v36 = [f'V{i}' for i in range(35, 37) if f'V{i}' in df.columns]\n",
    "        if v35_v36:\n",
    "            df['V35_V36_present'] = df[v35_v36].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        v37_v52 = [f'V{i}' for i in range(37, 53) if f'V{i}' in df.columns]\n",
    "        if v37_v52:\n",
    "            df['V37_V52_present'] = df[v37_v52].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        v53_v74 = [f'V{i}' for i in range(53, 75) if f'V{i}' in df.columns]\n",
    "        if v53_v74:\n",
    "            df['V53_V74_present'] = df[v53_v74].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        v75_v94 = [f'V{i}' for i in range(75, 95) if f'V{i}' in df.columns]\n",
    "        if v75_v94:\n",
    "            df['V75_V94_present'] = df[v75_v94].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        v95_v137 = [f'V{i}' for i in range(95, 138) if f'V{i}' in df.columns]\n",
    "        if v95_v137:\n",
    "            df['V95_V137_present'] = df[v95_v137].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        v138_v339 = [f'V{i}' for i in range(138, 340) if f'V{i}' in df.columns]\n",
    "        if v138_v339:\n",
    "            df['V138_V339_present'] = df[v138_v339].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        # 8. Transformaciones de tiempo (TransactionDT y D1-D15)\n",
    "        df['TransactionDT_days'] = df['TransactionDT'] / 86400\n",
    "        min_transaction_dt = df['TransactionDT'].min()\n",
    "        df['TransactionDT_normalized'] = df['TransactionDT'] - min_transaction_dt\n",
    "        df['TransactionDT_hour'] = (df['TransactionDT_normalized'] % 86400) // 3600\n",
    "        df['TransactionDT_day_of_week'] = (df['TransactionDT_normalized'] // 86400) % 7\n",
    "        df['TransactionDT_day_relative'] = df['TransactionDT_normalized'] // 86400\n",
    "\n",
    "        # Convertir a float32\n",
    "        df['TransactionDT_days'] = df['TransactionDT_days'].astype('float32')\n",
    "        df['TransactionDT_normalized'] = df['TransactionDT_normalized'].astype('float32')\n",
    "        df['TransactionDT_hour'] = df['TransactionDT_hour'].astype('float32')\n",
    "        df['TransactionDT_day_of_week'] = df['TransactionDT_day_of_week'].astype('float32')\n",
    "        df['TransactionDT_day_relative'] = df['TransactionDT_day_relative'].astype('float32')\n",
    "\n",
    "        # Diferencias y frecuencias\n",
    "        df['D2_minus_D1'] = (df['D2'] - df['D1']).astype('float32')\n",
    "        df['D2_over_D1'] = (df['D2'] / df['D1'].replace(0, np.nan)).astype('float32')\n",
    "        df['D1_frequency'] = (1 / df['D1'].replace(0, np.nan)).astype('float32')\n",
    "        df['D2_frequency'] = (1 / df['D2'].replace(0, np.nan)).astype('float32')\n",
    "\n",
    "        # Columnas binarias para presencia de D\n",
    "        d6_d9 = ['D6', 'D8', 'D9']\n",
    "        if all(col in df.columns for col in d6_d9):\n",
    "            df['D6_D9_present'] = df[d6_d9].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        d12_d14 = ['D12', 'D13', 'D14']\n",
    "        if all(col in df.columns for col in d12_d14):\n",
    "            df['D12_D14_present'] = df[d12_d14].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        d2_d3_d11 = ['D2', 'D3', 'D11']\n",
    "        if all(col in df.columns for col in d2_d3_d11):\n",
    "            df['D2_D3_D11_present'] = df[d2_d3_d11].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        for col in ['D1', 'D4', 'D5', 'D7', 'D10', 'D15', 'dist2']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_present'] = df[col].notnull().astype('int')\n",
    "\n",
    "        # 9. Crear addr_present\n",
    "        if all(col in df.columns for col in ['addr1', 'addr2']):\n",
    "            df['addr_present'] = df[['addr1', 'addr2']].notnull().any(axis=1).astype('int')\n",
    "\n",
    "        # 10. Eliminar columnas Vxxx de baja importancia\n",
    "        feature_importances = {\n",
    "            'R_emaildomain': 115, 'TransactionAmt': 111, 'C1': 110, 'C14': 96, 'card2': 93,\n",
    "            'card3': 83, 'card1': 81, 'D8': 79, 'C13': 75, 'ProductCD': 69, 'P_emaildomain': 69,\n",
    "            'TransactionDT': 63, 'id_02': 57, 'TransactionDT_day_relative': 57, 'D2': 55,\n",
    "            'id_20': 53, 'V156': 49, 'card6': 43, 'id_30': 42, 'V87': 37, 'V258': 36,\n",
    "            'V165': 34, 'C11': 33, 'addr1': 31, 'id_06': 28, 'id_14': 28, 'D4': 28, 'C3': 26,\n",
    "            'id_09': 25, 'V45': 25, 'D13': 24, 'id_01': 23, 'dist2': 22, 'id_17': 21,\n",
    "            'id_33': 21, 'D15': 20, 'V55': 20, 'TransactionDT_hour': 19, 'D3': 18, 'D14': 18,\n",
    "            'V189': 18, 'id_31': 17, 'DeviceInfo': 17, 'D6': 17, 'id_19': 16, 'C2': 16,\n",
    "            'card5': 15, 'D2_frequency': 15, 'id_03': 13, 'id_07': 13, 'id_18': 13, 'V308': 13,\n",
    "            'V56': 12, 'V58': 12, 'V310': 12, 'V314': 12, 'V315': 12, 'TransactionDT_days': 12,\n",
    "            'card4': 10, 'V139': 10, 'V251': 10, 'V261': 10, 'C12': 9, 'V67': 9, 'V74': 9,\n",
    "            'V78': 9, 'V207': 9, 'id_36': 8, 'V37': 8, 'V234': 8, 'id_10': 7, 'V149': 7,\n",
    "            'V152': 7, 'C4': 6, 'C8': 6, 'V143': 6, 'V158': 6, 'V169': 6, 'V206': 6, 'V245': 6,\n",
    "            'V332': 6, 'V333': 6, 'V338': 6, 'id_04': 5, 'id_32': 5, 'V57': 5, 'V137': 5,\n",
    "            'V145': 5, 'V159': 5, 'V162': 5, 'V166': 5, 'V217': 5, 'V221': 5, 'V224': 5,\n",
    "            'V263': 5, 'V280': 5, 'V309': 5, 'V336': 5, 'D1_frequency': 5, 'id_13': 4, 'D5': 4,\n",
    "            'V38': 4, 'V150': 4, 'V157': 4, 'V160': 4, 'V170': 4, 'V172': 4, 'V187': 4,\n",
    "            'V192': 4, 'V205': 4, 'V209': 4, 'V259': 4, 'V266': 4, 'V281': 4, 'V293': 4,\n",
    "            'V294': 4, 'V335': 4, 'V339': 4, 'P_emaildomain_present': 4, 'D7': 3, 'D9': 3,\n",
    "            'D10': 3, 'D12': 3, 'V44': 3, 'V52': 3, 'V77': 3, 'V79': 3, 'V99': 3, 'V109': 3,\n",
    "            'V130': 3, 'V161': 3, 'V164': 3, 'V201': 3, 'V208': 3, 'V229': 3, 'V243': 3,\n",
    "            'V257': 3, 'V271': 3, 'V312': 3, 'V313': 3, 'V324': 3, 'V327': 3,\n",
    "            'TransactionDT_day_of_week': 3, 'id_34': 2, 'C6': 2, 'C7': 2, 'V25': 2, 'V42': 2,\n",
    "            'V43': 2, 'V47': 2, 'V51': 2, 'V63': 2, 'V64': 2, 'V66': 2, 'V71': 2, 'V72': 2,\n",
    "            'V73': 2, 'V81': 2, 'V83': 2, 'V84': 2, 'V127': 2, 'V128': 2, 'V135': 2, 'V140': 2,\n",
    "            'V178': 2, 'V213': 2, 'V228': 2, 'V230': 2, 'V232': 2, 'V256': 2, 'V264': 2,\n",
    "            'V265': 2, 'V267': 2, 'V268': 2, 'V270': 2, 'V279': 2, 'V283': 2, 'V285': 2,\n",
    "            'V290': 2, 'V311': 2, 'V317': 2, 'V323': 2, 'V330': 2, 'D2_over_D1': 2, 'id_08': 1,\n",
    "            'id_11': 1, 'id_21': 1, 'id_24': 1, 'DeviceType': 1, 'V15': 1, 'V18': 1, 'V23': 1,\n",
    "            'V39': 1, 'V40': 1, 'V53': 1, 'V94': 1, 'V102': 1, 'V123': 1, 'V133': 1, 'V136': 1,\n",
    "            'V151': 1, 'V154': 1, 'V155': 1, 'V175': 1, 'V181': 1, 'V182': 1, 'V184': 1,\n",
    "            'V185': 1, 'V188': 1, 'V204': 1, 'V212': 1, 'V223': 1, 'V225': 1, 'V231': 1,\n",
    "            'V233': 1, 'V238': 1, 'V242': 1, 'V244': 1, 'V246': 1, 'V248': 1, 'V262': 1,\n",
    "            'V272': 1, 'V274': 1, 'V278': 1, 'V282': 1, 'V296': 1, 'V300': 1, 'V303': 1,\n",
    "            'V319': 1, 'V328': 1, 'V331': 1, 'V334': 1\n",
    "        }\n",
    "        low_importance_v_cols = [f'V{i}' for i in range(1, 340) if f'V{i}' in df.columns and f'V{i}' in feature_importances and feature_importances[f'V{i}'] <= 2]\n",
    "        df = df.drop(columns=low_importance_v_cols)\n",
    "\n",
    "        # 11. Convertir float64 a float32 e int64 a int32 para reducir el uso de memoria\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype('float32')\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype('int32')\n",
    "\n",
    "        # 12. Características derivadas\n",
    "        # 12.1. Proporción de fraudes por R_emaildomain\n",
    "        df['Fraud_Rate_R_emaildomain'] = df['R_emaildomain'].map(self.fraud_rate_by_remail).astype('float32')\n",
    "\n",
    "        # 12.2. Diferencia entre TransactionAmt y el promedio por card1\n",
    "        df['TransactionAmt_Diff_Avg_card1'] = (df['TransactionAmt'] - df['card1'].map(self.avg_amt_by_card1)).astype('float32')\n",
    "\n",
    "        # 12.3. Indicador de TransactionAmt extremo por card1\n",
    "        df['TransactionAmt_High_card1'] = (df['TransactionAmt'] > df['card1'].map(self.amt_percentile_90_by_card1)).astype('int')\n",
    "\n",
    "        # 12.4. Indicador de C14 alto\n",
    "        df['C14_High'] = (df['C14'] > self.c14_threshold).astype('int')\n",
    "\n",
    "        # 12.5. Frecuencia de transacciones por card1 en el último día\n",
    "        df['Transactions_Last_Day'] = np.zeros(len(df), dtype=np.float32)\n",
    "        grouped = df.groupby('card1')\n",
    "        for card, group in grouped:\n",
    "            group = group.sort_values('TransactionDT')\n",
    "            transaction_dt = group['TransactionDT'].values\n",
    "            indices = group.index\n",
    "            counts = np.zeros(len(group), dtype=np.float32)\n",
    "            for i in range(len(group)):\n",
    "                window_start = transaction_dt[i] - 86400\n",
    "                counts[i] = np.sum((transaction_dt >= window_start) & (transaction_dt <= transaction_dt[i]))\n",
    "            df.loc[indices, 'Transactions_Last_Day'] = counts\n",
    "\n",
    "        # 12.6. Diferencia D8 - D1\n",
    "        df['D8_minus_D1'] = (df['D8'] - df['D1']).astype('float32')\n",
    "\n",
    "        # 12.7. Suma de V156-V166\n",
    "        v156_166 = [f'V{i}' for i in range(156, 167) if f'V{i}' in df.columns]\n",
    "        if v156_166:\n",
    "            df['V156_V166_Sum'] = df[v156_166].sum(axis=1, skipna=True).astype('float32')\n",
    "\n",
    "        # Debug: Imprimir estadísticas de columnas clave para verificar\n",
    "        cols_to_check = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain',\n",
    "                         'TransactionAmt', 'Fraud_Rate_R_emaildomain', 'TransactionAmt_Diff_Avg_card1',\n",
    "                         'TransactionAmt_High_card1', 'C14_High', 'Transactions_Last_Day', 'D8_minus_D1',\n",
    "                         'V156_V166_Sum']\n",
    "        print(\"\\nEstadísticas de columnas clave después de la transformación:\")\n",
    "        for col in cols_to_check:\n",
    "            if col in df.columns:\n",
    "                print(f\"\\nColumna: {col}\")\n",
    "                print(f\"Mean: {df[col].mean()}\")\n",
    "                print(f\"Median: {df[col].median()}\")\n",
    "                print(f\"% Null: {df[col].isna().mean() * 100:.5f}%\")\n",
    "                print(f\"Valores únicos: {df[col].nunique()}\")\n",
    "                print(f\"Contenido único: {df[col].unique()[:10]}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        return self.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246c84f4-a44a-4ede-9520-9d07f2054d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('train_transaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36983aee-8e59-4546-8752-91ffc85b79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = TransactionCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32976008-3c52-4189-a300-04bc928a713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estadísticas calculadas en fit:\n",
      "avg_amt_by_card1:\n",
      "count    13553.000000\n",
      "mean       149.566359\n",
      "std        191.216012\n",
      "min          0.615000\n",
      "25%         59.000000\n",
      "50%        103.319024\n",
      "75%        167.857143\n",
      "max       3454.950000\n",
      "Name: TransactionAmt, dtype: float64\n",
      "\n",
      "amt_percentile_90_by_card1:\n",
      "count    13553.000000\n",
      "mean       227.080705\n",
      "std        285.958866\n",
      "min          0.615000\n",
      "25%         80.140400\n",
      "50%        150.000000\n",
      "75%        259.000000\n",
      "max       4645.150000\n",
      "Name: TransactionAmt, dtype: float64\n",
      "\n",
      "fraud_rate_by_remail:\n",
      "{0.0: 0.11898605918731002, 1.0: 0.04623475275728581, 2.0: 0.07648454374845083, 3.0: 0.16020408163265307, 4.0: 0.03485544447446636, 5.0: 0.02912952408787569, 6.0: 0.12875536480686695, 7.0: 0.011589403973509934, 8.0: 0.0, 9.0: 0.004830917874396135, 10.0: 0.0, 11.0: 0.03243636363636364}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TransactionCleaner at 0x28facfca690>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner.fit(train_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb52c13-c592-4c84-a73a-444927c28eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['P_emaildomain_present'] = df['P_emaildomain'].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['R_emaildomain_present'] = df['R_emaildomain'].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['M7_M9_present'] = df[m7_m9_cols].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['M1_M3_present'] = df[m1_m3_cols].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['V1_V11_present'] = df[v1_v11].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['V12_V34_present'] = df[v12_v34].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['V35_V36_present'] = df[v35_v36].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['V37_V52_present'] = df[v37_v52].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['V53_V74_present'] = df[v53_v74].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['V75_V94_present'] = df[v75_v94].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['V95_V137_present'] = df[v95_v137].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['V138_V339_present'] = df[v138_v339].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['TransactionDT_days'] = df['TransactionDT'] / 86400\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:197: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['TransactionDT_normalized'] = df['TransactionDT'] - min_transaction_dt\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['TransactionDT_hour'] = (df['TransactionDT_normalized'] % 86400) // 3600\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['TransactionDT_day_of_week'] = (df['TransactionDT_normalized'] // 86400) % 7\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['TransactionDT_day_relative'] = df['TransactionDT_normalized'] // 86400\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['D2_minus_D1'] = (df['D2'] - df['D1']).astype('float32')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['D2_over_D1'] = (df['D2'] / df['D1'].replace(0, np.nan)).astype('float32')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['D1_frequency'] = (1 / df['D1'].replace(0, np.nan)).astype('float32')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['D2_frequency'] = (1 / df['D2'].replace(0, np.nan)).astype('float32')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['D6_D9_present'] = df[d6_d9].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:222: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['D12_D14_present'] = df[d12_d14].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:226: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['D2_D3_D11_present'] = df[d2_d3_d11].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:230: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_present'] = df[col].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:230: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_present'] = df[col].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:230: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_present'] = df[col].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:230: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_present'] = df[col].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:230: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_present'] = df[col].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:230: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_present'] = df[col].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:230: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_present'] = df[col].notnull().astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:234: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['addr_present'] = df[['addr1', 'addr2']].notnull().any(axis=1).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:285: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Fraud_Rate_R_emaildomain'] = df['R_emaildomain'].map(self.fraud_rate_by_remail).astype('float32')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:288: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['TransactionAmt_Diff_Avg_card1'] = (df['TransactionAmt'] - df['card1'].map(self.avg_amt_by_card1)).astype('float32')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['TransactionAmt_High_card1'] = (df['TransactionAmt'] > df['card1'].map(self.amt_percentile_90_by_card1)).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['C14_High'] = (df['C14'] > self.c14_threshold).astype('int')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Transactions_Last_Day'] = np.zeros(len(df), dtype=np.float32)\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:310: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['D8_minus_D1'] = (df['D8'] - df['D1']).astype('float32')\n",
      "C:\\Users\\barba\\AppData\\Local\\Temp\\ipykernel_16380\\2418099851.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['V156_V166_Sum'] = df[v156_166].sum(axis=1, skipna=True).astype('float32')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estadísticas de columnas clave después de la transformación:\n",
      "\n",
      "Columna: ProductCD\n",
      "Mean: 0.6024011969566345\n",
      "Median: 0.0\n",
      "% Null: 0.00000%\n",
      "Valores únicos: 5\n",
      "Contenido único: [0. 1. 2. 3. 4.]\n",
      "\n",
      "Columna: card4\n",
      "Mean: 1.6702831983566284\n",
      "Median: 2.0\n",
      "% Null: 0.26704%\n",
      "Valores únicos: 4\n",
      "Contenido único: [ 0.  1.  2.  3. nan]\n",
      "\n",
      "Columna: card6\n",
      "Mean: 0.7471411824226379\n",
      "Median: 1.0\n",
      "% Null: 0.26603%\n",
      "Valores únicos: 4\n",
      "Contenido único: [ 0.  1. nan  2.  3.]\n",
      "\n",
      "Columna: P_emaildomain\n",
      "Mean: 1.9365551471710205\n",
      "Median: 1.0\n",
      "% Null: 15.99485%\n",
      "Valores únicos: 12\n",
      "Contenido único: [nan  0.  3.  1. 11.  5.  2.  8.  4.  7.]\n",
      "\n",
      "Columna: R_emaildomain\n",
      "Mean: 2.1874120235443115\n",
      "Median: 1.0\n",
      "% Null: 76.75162%\n",
      "Valores únicos: 12\n",
      "Contenido único: [nan  0.  2.  3.  5. 11.  9.  7.  6.  1.]\n",
      "\n",
      "Columna: TransactionAmt\n",
      "Mean: 135.02716064453125\n",
      "Median: 68.76899719238281\n",
      "% Null: 0.00000%\n",
      "Valores únicos: 20902\n",
      "Contenido único: [ 68.5    29.     59.     50.     49.    159.    422.5    15.    117.\n",
      "  75.887]\n",
      "\n",
      "Columna: Fraud_Rate_R_emaildomain\n",
      "Mean: 0.08177521824836731\n",
      "Median: 0.0764845460653305\n",
      "% Null: 76.75162%\n",
      "Valores únicos: 11\n",
      "Contenido único: [       nan 0.11898606 0.07648455 0.16020408 0.02912952 0.03243636\n",
      " 0.00483092 0.0115894  0.12875536 0.04623475]\n",
      "\n",
      "Columna: TransactionAmt_Diff_Avg_card1\n",
      "Mean: -7.408473834402685e-07\n",
      "Median: -33.81755828857422\n",
      "% Null: 0.00000%\n",
      "Valores únicos: 187335\n",
      "Contenido único: [-283.43115   -205.29276    -38.01554    -73.416336   -46.97222\n",
      "  -85.07143     57.119904   281.35535   -127.68341     -5.0204906]\n",
      "\n",
      "Columna: TransactionAmt_High_card1\n",
      "Mean: 0.10228943001320825\n",
      "Median: 0.0\n",
      "% Null: 0.00000%\n",
      "Valores únicos: 2\n",
      "Contenido único: [0 1]\n",
      "\n",
      "Columna: C14_High\n",
      "Mean: 0.09321129813391134\n",
      "Median: 0.0\n",
      "% Null: 0.00000%\n",
      "Valores únicos: 2\n",
      "Contenido único: [0 1]\n",
      "\n",
      "Columna: Transactions_Last_Day\n",
      "Mean: 19.804235458374023\n",
      "Median: 6.0\n",
      "% Null: 0.00000%\n",
      "Valores únicos: 881\n",
      "Contenido único: [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "\n",
      "Columna: D8_minus_D1\n",
      "Mean: 109.19709777832031\n",
      "Median: 13.041666030883789\n",
      "% Null: 87.33024%\n",
      "Valores únicos: 17156\n",
      "Contenido único: [  nan   82.   26.   21. -513.   83.  189.  126.  777.   12.]\n",
      "\n",
      "Columna: V156_V166_Sum\n",
      "Mean: 7447.82373046875\n",
      "Median: 0.0\n",
      "% Null: 0.00000%\n",
      "Valores únicos: 13023\n",
      "Contenido único: [0.0000000e+00 1.9375878e+05 1.9385878e+05 1.9388878e+05 1.9394878e+05\n",
      " 1.9406878e+05 1.4500000e+02 1.9383878e+05 1.9381378e+05 1.2000000e+02]\n"
     ]
    }
   ],
   "source": [
    "tr_train_cleaned_pipeline = cleaner.transform(train_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bca26433-4ac6-455d-9fa8-94b31043dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_train_cleaned_pipeline.to_csv('train_tr_cleaned_pipeline_to_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a5e5fef3-4926-467f-ab1d-84445a254672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to include statistical data and unique values? (yes/no):  yes\n",
      "Do you want to save the summary to an Excel file? (yes/no):  yes\n",
      "Enter the file name (without extension):  tr_train_cleaned_pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as tr_train_cleaned_pipeline.xlsx\n",
      "\n",
      "Summary DataFrame:\n",
      "                   Column Name Data Type  Percentage Null  Unique Values  \\\n",
      "0                TransactionID     int32          0.00000         590540   \n",
      "1                      isFraud     int32          0.00000              2   \n",
      "2                TransactionDT     int32          0.00000         573349   \n",
      "3               TransactionAmt   float32          0.00000          20902   \n",
      "4                    ProductCD   float32          0.00000              5   \n",
      "..                         ...       ...              ...            ...   \n",
      "348  TransactionAmt_High_card1     int32          0.00000              2   \n",
      "349                   C14_High     int32          0.00000              2   \n",
      "350      Transactions_Last_Day   float32          0.00000            881   \n",
      "351                D8_minus_D1   float32         87.33024          17156   \n",
      "352              V156_V166_Sum   float32          0.00000          13023   \n",
      "\n",
      "              Min           Max          Mean        Median           Std  \\\n",
      "0    2.987000e+06  3.577539e+06  3.282270e+06  3.282270e+06  1.704744e+05   \n",
      "1    0.000000e+00  1.000000e+00  3.499001e-02  0.000000e+00  1.837546e-01   \n",
      "2    8.640000e+04  1.581113e+07  7.372311e+06  7.306528e+06  4.617224e+06   \n",
      "3    2.510000e-01  3.193739e+04  1.350272e+02  6.876900e+01  2.391627e+02   \n",
      "4    0.000000e+00  4.000000e+00  6.024012e-01  0.000000e+00  1.167801e+00   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "348  0.000000e+00  1.000000e+00  1.022894e-01  0.000000e+00  3.030288e-01   \n",
      "349  0.000000e+00  1.000000e+00  9.321130e-02  0.000000e+00  2.907286e-01   \n",
      "350  1.000000e+00  8.810000e+02  1.980424e+01  6.000000e+00  4.563472e+01   \n",
      "351 -6.374167e+02  1.707792e+03  1.091971e+02  1.304167e+01  2.499484e+02   \n",
      "352  0.000000e+00  6.797224e+05  7.447824e+03  0.000000e+00  6.016157e+04   \n",
      "\n",
      "                Unique Content  \n",
      "0                               \n",
      "1                       [0, 1]  \n",
      "2                               \n",
      "3                               \n",
      "4    [0.0, 1.0, 2.0, 3.0, 4.0]  \n",
      "..                         ...  \n",
      "348                     [0, 1]  \n",
      "349                     [0, 1]  \n",
      "350                             \n",
      "351                             \n",
      "352                             \n",
      "\n",
      "[353 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "create_summary_excel(tr_train_cleaned_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "48e4b172-b493-456c-806a-41b7d5325ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline entrenado y guardado como 'transaction_cleaner_trained.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Guardar el pipeline entrenado en un archivo\n",
    "joblib.dump(cleaner, 'transaction_cleaner_trained.pkl')\n",
    "print(\"Pipeline entrenado y guardado como 'transaction_cleaner_trained.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f555665-5975-4c98-96d6-dc338f66dc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
